{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dimcli\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import gensim\n",
    "import spacy\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "np.random.seed(123)\n",
    "import pickle\n",
    "nltk.download('wordnet')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('test/final_hdsi_faculty_updated.csv')\n",
    "df = pd.read_csv('test/test.csv')\n",
    "authors = df[['authors']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = str(authors.loc[0][0])\n",
    "fg = list(eval(test))#[0]['first_name']\n",
    "lis = []\n",
    "lis2 = []\n",
    "for i in fg:\n",
    "    if 'first_name' in i:\n",
    "        first = i['first_name']\n",
    "        last = i['last_name']\n",
    "        full = first + \" \" + last\n",
    "        #print(full)\n",
    "        lis.append(full)\n",
    "        ids = i['researcher_id']\n",
    "        #print(ids)\n",
    "        lis2.append(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new list to collect names\n",
    "new = []\n",
    "#new list to collect corresponding ids\n",
    "new2 = []\n",
    "#looping through length of author column\n",
    "for i in range(len(authors)):\n",
    "    #turning string of list of dictionaries into list of dictionaries\n",
    "    temp = list(eval(authors.loc[i][0]))\n",
    "    #names\n",
    "    lis = []\n",
    "    #ids\n",
    "    lis2 = []\n",
    "    #looping through the list of dictionaries\n",
    "    for i in temp:\n",
    "        if 'first_name' in i:\n",
    "            first = i['first_name']\n",
    "            last = i['last_name']\n",
    "            #concatenating first and last name\n",
    "            full = first + \" \" + last\n",
    "            lis.append(full)\n",
    "            #print(lis)\n",
    "            ids = i['researcher_id']\n",
    "            lis2.append(ids)\n",
    "        else:\n",
    "            lis.append(i)\n",
    "            lis2.append(i)\n",
    "    new.append(lis)\n",
    "    new2.append(lis2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding new column, \"names,\" to the original dataframe\n",
    "names = pd.Series(new)\n",
    "df['names'] = names.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding new column, \"ids,\" to the original dataframe\n",
    "ids = pd.Series(new2)\n",
    "df['ids'] = ids.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate data by researcher-year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2 = df.explode(['names', 'ids']).reset_index(drop=True)\n",
    "df2 = df.apply(pd.Series.explode).reset_index(drop=True)\n",
    "\n",
    "testing = df2['ids'].value_counts()\n",
    "#print(testing.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdsi = pd.read_csv(\"HDSI.csv\")\n",
    "faculty = hdsi[hdsi['Dimensions ID'] != 'no ID']['Dimensions ID']\n",
    "#manually adding professors since they do not have dimensions ids\n",
    "add = pd.Series(['Aaron McMillan Fraenkel', 'Justin Eldridge'])\n",
    "faculty = list(faculty.append(add))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaned out all names & ids that do not match our hdsi faculty list\n",
    "df3 = df2[df2.ids.isin(faculty)].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df3['ids'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['abstract'] = df3['abstract'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "redundant = ['abstract', 'purpose', 'paper', 'goal', 'usepackage', 'cod']\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return WordNetLemmatizer().lemmatize(text, pos='v')\n",
    "def preprocess_abstract(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3 and token not in redundant:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return \" \".join(result)\n",
    "\n",
    "\n",
    "df3['abstract_processed'] = df3['abstract'].apply(preprocess_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df3[df3['year'] >= 2015]\n",
    "counts = CountVectorizer().fit_transform(df3['abstract_processed'])\n",
    "authors = {}\n",
    "for author in df3.names.unique():\n",
    "    authors[author] = {\n",
    "        2015 : list(),\n",
    "        2016 : list(),\n",
    "        2017 : list(),\n",
    "        2018 : list(),\n",
    "        2019 : list(),\n",
    "        2020 : list(),\n",
    "        2021 : list()\n",
    "    }\n",
    "for i, row in df3.iterrows():\n",
    "    authors[row['names']][row['year']].append(row['abstract_processed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "381"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_docs = []\n",
    "missing_author_years = {author : list() for author in df3.names.unique()}\n",
    "for author, author_dict in authors.items():\n",
    "    for year, documents in author_dict.items():\n",
    "        if len(documents) == 0:\n",
    "            missing_author_years[author].append(year)\n",
    "            continue\n",
    "        all_docs.append(\" \".join(documents))\n",
    "len(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initate LDA model\n",
    "countVec = CountVectorizer()\n",
    "counts = countVec.fit_transform(all_docs)\n",
    "names = countVec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n"
     ]
    }
   ],
   "source": [
    "modeller = LatentDirichletAllocation(n_components=10, n_jobs=-1, random_state=123)\n",
    "result = modeller.fit_transform(counts)\n",
    "modeller2 = LatentDirichletAllocation(n_components=20, n_jobs=-1, random_state=123)\n",
    "result2 = modeller2.fit_transform(counts)\n",
    "modeller3 = LatentDirichletAllocation(n_components=30, n_jobs=-1, random_state=123)\n",
    "result3 = modeller3.fit_transform(counts)\n",
    "modeller4 = LatentDirichletAllocation(n_components=40, n_jobs=-1, random_state=123)\n",
    "result4 = modeller4.fit_transform(counts)\n",
    "modeller5 = LatentDirichletAllocation(n_components=50, n_jobs=-1, random_state=123)\n",
    "result5 = modeller5.fit_transform(counts)\n",
    "\n",
    "models = {'10':modeller,'20':modeller2,'30':modeller3,'40':modeller4,'50':modeller5}\n",
    "results = {'10':result,'20':result2,'30':result3,'40':result4,'50':result5}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "topicnames = {\n",
    "    num_topics : [\"Topic\" + str(i) for i in range(num_topics)] for num_topics in range(10, 60, 10)\n",
    "}\n",
    "\n",
    "# index names\n",
    "docnames = [\"Doc\" + str(i) for i in range(len(all_docs))]\n",
    "\n",
    "# Make the pandas dataframe\n",
    "df_document_topic = {\n",
    "    num_topics : pd.DataFrame(results[f'{num_topics}'], columns=topicnames[num_topics], index=docnames) for num_topics in range(10, 60, 10)\n",
    "}\n",
    "\n",
    "# Get dominant topic for each document\n",
    "dominant_topic = {\n",
    "    num_topics : np.argmax(df_document_topic[num_topics].values, axis=1) for num_topics in range(10, 60, 10)\n",
    "}\n",
    "\n",
    "for num_topics, df in df_document_topic.items():\n",
    "    df['dominant_topic'] = dominant_topic[num_topics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{10: array([2, 2, 8, 8, 2, 0, 2, 2, 1, 9, 1, 7, 9, 5, 2, 2, 2, 2, 9, 9, 8, 9,\n",
       "        9, 8, 8, 8, 1, 1, 9, 1, 2, 2, 2, 9, 2, 2, 0, 0, 0, 0, 0, 0, 2, 2,\n",
       "        9, 9, 2, 9, 9, 9, 5, 9, 3, 3, 5, 5, 8, 9, 0, 4, 2, 2, 2, 2, 2, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 9, 9, 9, 9, 9, 1, 3, 1, 1, 1, 1,\n",
       "        9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 9, 9,\n",
       "        9, 9, 9, 9, 9, 6, 6, 6, 6, 6, 6, 6, 0, 8, 8, 8, 8, 8, 8, 9, 9, 9,\n",
       "        9, 9, 9, 9, 9, 2, 2, 9, 2, 1, 7, 2, 2, 0, 2, 2, 0, 8, 2, 9, 9, 9,\n",
       "        5, 9, 9, 4, 4, 4, 7, 7, 5, 9, 9, 5, 5, 9, 5, 9, 9, 5, 5, 5, 5, 2,\n",
       "        7, 5, 2, 2, 2, 2, 5, 2, 2, 2, 2, 2, 2, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "        8, 2, 0, 0, 0, 0, 0, 5, 5, 5, 2, 2, 2, 5, 5, 5, 5, 5, 5, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 9, 9, 9, 9, 9, 9, 4, 4, 7, 4, 3, 4, 4, 9,\n",
       "        5, 9, 9, 9, 9, 9, 9, 0, 0, 4, 9, 4, 4, 4, 0, 0, 0, 4, 4, 4, 4, 4,\n",
       "        9, 9, 5, 5, 9, 5, 9, 9, 3, 9, 9, 4, 4, 7, 0, 4, 5, 5, 5, 2, 2, 5,\n",
       "        5, 5, 5, 1, 1, 5, 5, 2, 5, 2, 2, 2, 2, 5, 2, 9, 5, 5, 1, 7, 7, 7,\n",
       "        7, 7, 7, 7, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 5, 0, 2, 2, 3, 2, 9,\n",
       "        2, 2, 9, 9, 2, 2, 2, 0, 0, 0, 0, 5, 3, 9, 5, 9, 5, 9, 9, 5, 0, 9,\n",
       "        0, 2, 2, 0, 6, 6, 1, 0, 0, 1, 1, 1, 1, 1, 1, 7, 7, 2, 0, 9, 2, 9,\n",
       "        1, 9, 0, 0, 9, 9, 8], dtype=int64),\n",
       " 20: array([ 2,  2,  2,  8,  2,  0,  2,  2, 13, 18, 12, 18, 18, 18,  2,  2,  2,\n",
       "         2,  2,  2,  9,  9,  2,  9,  8,  8, 16, 19,  9,  1,  5,  2, 14, 14,\n",
       "         2, 15,  0,  0,  0,  0,  0,  0,  2,  2,  2,  2,  2,  1,  2,  1,  2,\n",
       "        14,  3, 15,  3,  5,  8,  1,  0,  4,  2,  2, 15,  2, 17,  0,  0,  0,\n",
       "        10, 17, 12,  0, 15, 15,  0, 17,  2,  2, 13,  9,  9,  2, 12, 12, 12,\n",
       "        12, 16, 16,  9, 15,  7, 16, 17, 17, 16,  2, 14,  1,  2, 17,  1,  1,\n",
       "        17,  1, 18,  9,  4, 16,  2,  2,  9,  9,  9,  9,  9,  6,  6,  6,  6,\n",
       "         6,  6,  6,  0,  8,  8,  8,  8,  2,  2,  7,  5,  9,  5,  9, 11, 11,\n",
       "         2,  2, 15,  9, 17,  1, 12, 17,  9,  0, 15, 17,  0, 15, 18, 10, 10,\n",
       "        10, 10, 10, 10, 16,  4, 16,  4, 17, 16, 15, 16, 16,  7, 14, 16, 14,\n",
       "        14, 19, 19, 19, 19, 19,  9, 19, 19, 19, 19, 19, 19, 17, 17,  2, 18,\n",
       "        18, 17,  1,  9,  9,  9,  9,  1,  9,  9,  9, 13, 18,  0,  0,  0, 18,\n",
       "         0, 15, 19,  5, 18, 17, 17,  2, 15, 15, 15, 15, 10, 16,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  9,  9,  9,  9,  9,  9,  9,  4,  4,  4,\n",
       "         3,  0,  4,  9, 15,  9,  9,  9,  9, 14,  9,  0,  0,  4, 17,  4,  4,\n",
       "         4,  0,  0,  0, 10,  4,  4,  6,  4, 11,  5,  5,  5,  5,  5,  5,  5,\n",
       "         3, 15,  5,  4, 16, 16,  0,  4,  5,  5,  5,  5,  2,  5,  5,  5, 19,\n",
       "         1, 16, 17,  5, 14,  5,  5,  1,  1,  0, 11,  5,  2,  2,  2,  1,  7,\n",
       "        10, 10, 10,  4,  4,  4,  0, 17,  9, 17,  2,  2,  2,  2,  2,  2,  2,\n",
       "        12,  0, 13, 14, 14,  2,  9, 13,  2, 16,  9, 13, 13,  2,  0,  0,  0,\n",
       "         0, 15,  3, 16,  5,  5,  5,  5,  5, 15,  0,  5,  0, 16, 19,  0,  6,\n",
       "        12, 12,  0,  0, 16, 16, 16, 16, 16, 16, 16, 16, 16,  0, 15, 10, 11,\n",
       "        18,  2,  0,  0,  2,  2,  2], dtype=int64),\n",
       " 30: array([29, 29,  8, 29, 29,  0, 24, 24, 21, 21, 12, 21, 21, 18,  2,  2, 11,\n",
       "         2, 21, 21,  9,  2, 11,  8,  8,  8,  5, 11, 21,  1, 29, 24, 24, 24,\n",
       "        24, 24, 10,  0, 10, 10, 10, 10, 16, 21, 24, 21,  2, 21, 24, 24, 11,\n",
       "        24,  3,  3,  5,  5,  8, 21,  0,  1, 15, 15, 24, 24, 24,  0, 20, 25,\n",
       "        18, 17, 25,  0, 25, 15,  0, 17, 24, 24, 13, 24, 24, 24, 12, 12, 12,\n",
       "        16, 16, 25, 27, 15,  7, 16, 23, 23, 16, 16, 14, 24, 29, 17, 26, 26,\n",
       "        26, 26, 18,  2, 22, 21, 21, 21, 21, 21, 21, 21, 21,  6, 19, 19, 19,\n",
       "         6,  6,  6,  0, 21,  8,  8,  8,  2,  2, 23, 14, 21,  9, 21,  9,  9,\n",
       "        21, 24, 27,  9, 24, 16, 20, 16, 29,  0, 15, 29,  0,  8, 18, 23, 23,\n",
       "         9, 10, 10, 15, 20, 20, 16,  6, 17, 16, 26, 16, 16,  7, 14, 16, 14,\n",
       "        14, 11, 11, 11, 24, 24,  6, 24, 29, 24, 24, 24, 11, 16, 18, 18, 18,\n",
       "        18, 17,  1, 24, 21, 21, 24,  1, 21, 21, 21, 13, 18,  0, 10, 10, 10,\n",
       "        10, 15, 29,  5, 18, 29, 18, 29, 21, 21, 29, 29, 29, 10,  0,  0, 10,\n",
       "        10, 10,  0, 10, 25, 10,  0, 21, 21, 21, 21, 21, 21, 28,  0,  7, 25,\n",
       "        10, 17,  3,  9,  4,  9,  9,  9, 15, 14, 28,  0,  0,  4, 17,  4,  4,\n",
       "         4,  0,  0,  0,  9,  4,  4,  6,  4,  9,  9,  9, 29, 29, 29,  9,  9,\n",
       "         3, 29, 25,  4, 16, 16,  0, 20, 24, 24,  5, 24,  2,  5, 24,  5, 24,\n",
       "         5, 23, 17, 24, 15,  5, 24,  7,  1,  0,  4,  5, 24,  2,  2,  1,  7,\n",
       "        17, 10, 20, 20, 20, 20,  0, 29, 29, 29,  2,  2,  2,  2,  2,  2, 21,\n",
       "        16,  0, 14, 14, 14,  2,  9, 21,  2, 16,  9, 21, 21, 24,  0,  0,  0,\n",
       "         0, 25,  3, 16,  5,  3, 28,  9,  3,  9,  0, 24,  0, 29, 29,  0,  6,\n",
       "        20, 25,  0,  0, 16, 16, 16, 16, 16,  1, 16, 16, 16, 25, 25, 24, 11,\n",
       "        18, 11,  0,  0, 24, 24, 21], dtype=int64),\n",
       " 40: array([29, 29, 31, 29, 29,  0, 24,  2, 13, 21, 12, 21, 21, 18,  2,  2,  2,\n",
       "         2, 21, 21,  9,  9, 19,  9,  8,  8,  5, 19, 21,  1, 29, 15, 29, 24,\n",
       "        24, 15, 10, 10, 10, 10, 10, 10, 16, 24, 24, 21,  2,  1,  1,  1, 11,\n",
       "        18,  3,  3,  5,  5, 29,  1,  0,  0, 30, 32, 24, 32, 30,  0, 20, 25,\n",
       "        34, 18, 25,  0, 25, 25,  0, 25, 22, 22, 23, 29, 21,  2, 12, 12, 37,\n",
       "         2, 16, 34, 22, 14,  7, 39, 23, 23, 32,  2, 38, 25, 29, 17, 26, 26,\n",
       "        17, 26, 31, 32,  4, 21, 21, 21, 21, 35, 21, 35, 35,  6, 37, 37, 19,\n",
       "        37,  6,  6,  0, 21, 21, 21, 21, 21,  2, 23, 11, 21, 29, 21,  9,  9,\n",
       "        39, 39, 39, 39, 24,  1, 12, 35, 26,  0, 19, 29,  0,  8, 18, 15, 15,\n",
       "        15, 15, 10, 15, 16, 14, 16,  6, 24, 16, 36, 34, 16, 16, 14, 16, 14,\n",
       "        14, 32, 32, 32, 32, 32, 10, 32, 29, 32, 32, 32, 32, 18, 18, 18, 18,\n",
       "        18, 17, 30,  9, 21,  9, 38, 30, 35, 21, 21, 13, 18, 10,  0, 10, 18,\n",
       "        10, 15, 34,  5, 18, 18, 36, 29, 21, 21, 29, 29, 29, 16,  0,  0, 10,\n",
       "        25, 10,  0, 13, 25, 10,  0, 34, 34, 34, 34, 34, 21, 28,  0,  7, 20,\n",
       "        10,  0, 33,  9,  4, 35,  9,  9, 21, 14, 35,  0,  0,  4, 30,  4,  4,\n",
       "         4,  0,  0,  0, 10,  4,  4,  6,  4, 11, 11,  9, 29, 29, 29, 27,  9,\n",
       "         3, 29, 25,  4, 16, 16,  0, 10, 24, 24, 24, 24,  2, 24,  5, 24, 29,\n",
       "        32, 32, 27, 24, 38,  5, 27, 32,  1, 24, 36,  5, 24, 15, 15,  1, 32,\n",
       "        10, 30, 28, 20, 20, 29,  0, 36, 36, 36, 15, 15, 15,  2,  2, 21, 21,\n",
       "        20,  0, 27, 24, 31,  2,  9, 27,  2, 16,  9, 27, 27, 18,  0,  0,  0,\n",
       "         0, 15, 36, 16,  5, 24, 34, 34,  3, 35,  0, 24,  0, 29, 29,  0, 14,\n",
       "        30, 25,  0,  0, 16, 36, 16, 16, 16,  1, 16, 16, 36, 35, 28, 24, 11,\n",
       "        18, 24,  0,  0, 21, 21,  2], dtype=int64),\n",
       " 50: array([29, 29, 31, 29, 29,  0, 24, 24, 21, 21, 47, 21, 21, 43,  2,  2,  2,\n",
       "         2, 21, 21,  9,  9, 35,  9,  8,  8, 36, 36, 49,  1, 24, 24, 24, 24,\n",
       "        24, 24, 47, 47, 10, 10, 10, 47, 16, 24, 21, 21, 24,  1, 39, 39,  5,\n",
       "        41,  3,  3,  5,  3,  8, 21,  0, 31, 32, 32, 24, 32, 30,  0, 20, 25,\n",
       "        34, 17, 25,  0, 15, 15,  0, 17, 14, 14, 23, 21, 21,  2, 12, 12, 15,\n",
       "        16, 16, 47, 35, 40, 40, 40, 43, 43, 40, 18, 38,  1, 17, 17, 26, 26,\n",
       "        17, 20, 30, 32, 16, 21, 21, 21, 21, 21, 21, 21, 21,  6, 19, 25, 25,\n",
       "        37,  6,  6,  0, 41, 41,  8,  8, 41,  2, 23, 45, 21, 39, 21, 39, 39,\n",
       "        21, 39, 39,  9, 43,  1, 20, 30, 19,  0, 19, 30,  0, 33, 17, 38, 38,\n",
       "        38, 10, 10, 38, 42, 30, 16, 42, 30, 16, 20, 48, 44, 30, 14, 44, 14,\n",
       "        14, 41, 32, 41, 43, 24,  7, 43, 29, 32, 43, 43, 41, 43, 43, 43, 43,\n",
       "        43, 43, 49, 49, 21, 49, 49, 49, 21, 21, 21, 13, 43, 22, 47, 10, 18,\n",
       "        10, 29, 23, 48, 43, 43, 43, 29, 21, 21, 29, 29, 29, 19,  0,  0, 47,\n",
       "        47, 10,  0, 22, 47, 47,  0, 34, 34, 34, 34, 34, 34, 28, 47,  7, 47,\n",
       "        47, 47, 47, 48,  4, 48,  9,  9, 21, 20, 48,  0,  0, 18, 30,  4, 46,\n",
       "        46,  0,  0,  0, 45, 46, 46, 35, 46, 11, 29, 29, 29, 29, 29, 29, 48,\n",
       "        48, 29, 25,  5, 16, 42,  0, 42, 24, 24, 24, 30, 30, 43, 24, 49, 24,\n",
       "        32, 40, 17, 32, 38, 20, 24, 49,  1, 49, 49,  5, 24,  5,  5,  1, 32,\n",
       "        42, 42, 42, 42, 42, 42,  0,  9,  9, 43,  2,  2,  2,  2,  2,  4,  2,\n",
       "        16,  0, 24, 24, 21,  2,  9, 27,  2, 16,  9, 27, 27, 24,  0,  0,  0,\n",
       "         0, 43,  3, 29,  5, 24, 34, 39, 24,  5,  0, 24,  0, 43, 43,  0, 16,\n",
       "        47, 15,  0,  0, 40, 40, 40, 40, 40,  1, 40, 40, 40, 25, 28, 24, 48,\n",
       "        42, 24,  0,  0, 39, 35, 21], dtype=int64)}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dominant_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_list = []\n",
    "year_list = []\n",
    "for author in authors.keys():\n",
    "    for i in range(7):\n",
    "        if (2015 + i) not in missing_author_years[author]:\n",
    "            author_list.append(author)\n",
    "            year_list.append(2015 + i)\n",
    "\n",
    "for df in df_document_topic.values():\n",
    "    df['author'] = author_list\n",
    "    df['year'] = year_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged = {\n",
    "    num_topics : df_document_topic[num_topics].groupby('author').mean().drop(['dominant_topic', 'year'], axis=1) for num_topics in df_document_topic.keys()\n",
    "}\n",
    "\n",
    "filtered = {\n",
    "    threshold : {num_topics : averaged[num_topics].mask(averaged[num_topics] < threshold, other=0) for num_topics in averaged.keys()} for threshold in [.1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {}\n",
    "for num_topics in range(10, 60, 10):\n",
    "    labels[num_topics] = filtered[.1][num_topics].index.to_list()\n",
    "    labels[num_topics].extend(filtered[.1][num_topics].columns.to_list())\n",
    "\n",
    "\n",
    "sources = {threshold : {} for threshold in [.1]}\n",
    "targets = {threshold : {} for threshold in [.1]}\n",
    "values = {threshold : {} for threshold in [.1]}\n",
    "\n",
    "for threshold in [.1]:\n",
    "    for num_topics in range(10, 60, 10):\n",
    "        curr_sources = []\n",
    "        curr_targets = []\n",
    "        curr_values = []\n",
    "        index_counter = 0\n",
    "        for index, row in filtered[threshold][num_topics].iterrows():\n",
    "            for i, value in enumerate(row):\n",
    "                if value != 0:\n",
    "                    curr_sources.append(index_counter)\n",
    "                    curr_targets.append(108 + i)\n",
    "                    curr_values.append(value)\n",
    "            index_counter += 1\n",
    "        sources[threshold][num_topics] = curr_sources\n",
    "        targets[threshold][num_topics] = curr_targets\n",
    "        values[threshold][num_topics] = curr_values\n",
    "\n",
    "positions = {\n",
    "    num_topics : {label : i for i, label in enumerate(labels[num_topics])} for num_topics in averaged.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_ranks(array):\n",
    "    ranks = []\n",
    "    for value in array:\n",
    "        for i, percentage in enumerate(np.arange(.1, 1.1, .1)):\n",
    "            if value <= np.quantile(array, percentage):\n",
    "                ranks.append(i + 1)\n",
    "                break\n",
    "    return ranks\n",
    "\n",
    "final_values = {threshold : {} for threshold in [.1]}\n",
    "\n",
    "for threshold in [.1]:\n",
    "    for num_topics in range(10, 60, 10):\n",
    "        curr_values_array = np.array(values[threshold][num_topics])\n",
    "        final_values[threshold][num_topics] = split_into_ranks(curr_values_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics_list(model, feature_names, no_top_words):\n",
    "    topic_list = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topic_list.append(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "    return topic_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_labels = {}\n",
    "for num_topics in range(10, 60, 10):\n",
    "    link_labels[num_topics] = labels[num_topics].copy()\n",
    "    link_labels[num_topics][50:] = display_topics_list(models[f'{num_topics}'], names, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:104: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n"
     ]
    }
   ],
   "source": [
    "counts = CountVectorizer().fit_transform(df3['abstract_processed'])\n",
    "transformed_list = []\n",
    "for model in models.values():\n",
    "    transformed_list.append(model.transform(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {threshold : {} for threshold in [.1]}\n",
    "for i, matrix in enumerate(transformed_list):\n",
    "    for threshold in [.1]:\n",
    "        df = pd.DataFrame(matrix)\n",
    "        df.mask(df < threshold, other=0, inplace=True)\n",
    "        df['author'] = df3['names']\n",
    "        df['year'] = df3['year']\n",
    "        df['citations'] = df3['times_cited'] + 1\n",
    "\n",
    "        # noralization of citations: Scaling to a range [0, 1]\n",
    "        df['citations_norm'] = df.groupby(by=['author', 'year'])['citations'].apply(lambda x: (x-x.min())/(x.max()-x.min()))#normalize_by_group(df=df, by=['author', 'year'])['citations']\n",
    "        df['abstract'] = df3['abstract']\n",
    "        df['title'] = df3['title']\n",
    "        df.fillna(1, inplace=True)\n",
    "        \n",
    "        #alpha weight parameter for weighting importance of citations vs topic relation\n",
    "        alpha = .75\n",
    "        for topic_num in range((i+1) * 10):\n",
    "            df[f'{topic_num}_relevance'] = alpha * df[topic_num] + (1-alpha) * df['citations_norm']\n",
    "        dataframes[threshold][(i+1) * 10] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_top_list(data_frame, num_topics, threshold):\n",
    "    top_5s = []\n",
    "    the_filter = filtered[threshold][num_topics]\n",
    "    for topic in range(num_topics):\n",
    "        relevant = the_filter[the_filter[f'Topic{topic}'] != 0].index.to_list()\n",
    "        to_append = data_frame[data_frame[f'{topic}_relevance'] > 0].reset_index()\n",
    "        to_append = to_append[to_append['author'].isin(relevant)].reset_index()\n",
    "        top_5s.append(to_append) \n",
    "    return top_5s\n",
    "\n",
    "tops = {\n",
    "    threshold : {num_topics : create_top_list(dataframes[threshold][num_topics], num_topics, threshold) for num_topics in range(10, 60, 10)} for threshold in [.1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: \n",
      "The dash_core_components package is deprecated. Please replace\n",
      "`import dash_core_components as dcc` with `from dash import dcc`\n",
      "  \n",
      "C:\\Users\\brian\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: \n",
      "The dash_html_components package is deprecated. Please replace\n",
      "`import dash_html_components as html` with `from dash import html`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "import dash_bootstrap_components as dbc\n",
    "import plotly.graph_objects as go\n",
    "from dash.dependencies import Input, Output, State\n",
    "\n",
    "# sankey diagrams for diff numbers of topics\n",
    "\n",
    "heights = {\n",
    "  10 : 1000,\n",
    "  20 : 1500,\n",
    "  30 : 2000,\n",
    "  40 : 2500,\n",
    "  50 : 3000\n",
    "}\n",
    "\n",
    "figs = {threshold : {} for threshold in [.1]}\n",
    "for threshold in [.1]:\n",
    "    for num_topics in range(10, 60, 10):\n",
    "        fig = go.Figure(data=[go.Sankey(\n",
    "            node = dict(\n",
    "                pad = 15,\n",
    "                thickness = 20,\n",
    "                line = dict(color = 'black', width = 0.5),\n",
    "                label = labels[num_topics],\n",
    "                color = ['#666699' for i in range(len(labels[num_topics]))],\n",
    "                customdata = link_labels[num_topics],\n",
    "                hovertemplate='%{customdata} Total Flow: %{value}<extra></extra>'\n",
    "            ),\n",
    "            link = dict(\n",
    "                color = ['rgba(204, 204, 204, .5)' for i in range(len(sources[threshold][num_topics]))],\n",
    "                source = sources[threshold][num_topics],\n",
    "                target = targets[threshold][num_topics],\n",
    "                value = final_values[threshold][num_topics]\n",
    "            )\n",
    "        )])\n",
    "        fig.update_layout(title_text=\"Author Topic Connections\", font=dict(size = 10, color = 'white'), height=heights[num_topics], paper_bgcolor=\"black\", plot_bgcolor='black')\n",
    "        figs[threshold][num_topics] = fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = {\n",
    "    10 : display_topics_list(models['10'], names, 10),\n",
    "    20 : display_topics_list(models['20'], names, 10),\n",
    "    30 : display_topics_list(models['30'], names, 10),\n",
    "    40 : display_topics_list(models['40'], names, 10),\n",
    "    50 : display_topics_list(models['50'], names, 10)\n",
    "}\n",
    "\n",
    "combined = pd.read_csv('final_hdsi_faculty_updated.csv')\n",
    "#combined[combined.title == 'Unperturbed: spectral analysis beyond Davis-Kahan'].abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = {}\n",
    "for i, word in enumerate(names):\n",
    "    locations[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{10: ['microbiome microbial sample study sequence data diversity associate human microbiota',\n",
       "  'data model patients health flow methods result base risk research',\n",
       "  'data model learn network base information propose cod provide approach',\n",
       "  'network robustness neural model base adversarial certify methods time state',\n",
       "  'cell type brain cells human gene expression single neurons regulatory',\n",
       "  'learn model time network propose approach process base result state',\n",
       "  'patients data study model clinical cancer treatment measure result survival',\n",
       "  'genome data sequence ecdna identify cancer patients high read genes',\n",
       "  'distance graph algorithm result scheme problem bound approximation time measurements',\n",
       "  'model data neural time method brain network base test result'],\n",
       " 20: ['microbiome microbial study sample sequence diversity data associate microbiota human',\n",
       "  'model data prediction methods learn base train test performance bootstrap',\n",
       "  'model propose graph cod network data base learn problem scheme',\n",
       "  'network robustness model neural adversarial certify approximation function time propose',\n",
       "  'cell type cells genes genome identify single expression ecdna brain',\n",
       "  'model learn brain approach base network synaptic neural method neurons',\n",
       "  'patients model study data clinical cancer treatment survival measure pregnancy',\n",
       "  'level conflict work model quantitative help specific events pain kinds',\n",
       "  'distance result quantization measurements edit time algorithm reconstruction signal compress',\n",
       "  'model test problem propose data result base method sample distribution',\n",
       "  'sample data sequence human study neural result time power approach',\n",
       "  'model neural network spike dynamics neurons brain scale activity population',\n",
       "  'flow image cardiac pulmonary patients mean automate volume compare artery',\n",
       "  'mesh protein position cellular surface simulations process uncertainty diffusion result',\n",
       "  'data brain task cluster methods signal dimensional time approach activity',\n",
       "  'network model learn propose time algorithm state base data problem',\n",
       "  'data patients model research result base health study methods risk',\n",
       "  'data pattern learn scientific workflow scale systems compute workflows science',\n",
       "  'data causal model time workflow network learn dynamic approach dynamics',\n",
       "  'time build data applications energy design process real reduce model'],\n",
       " 30: ['microbial study microbiome data sequence sample bacterial microbiota diversity result',\n",
       "  'model data bootstrap prediction test methods base regression result intervals',\n",
       "  'cod scheme capacity graph rate problem channel information network study',\n",
       "  'network robustness neural model approximation general adversarial layer time function',\n",
       "  'cell type brain regulatory single methylation gene cellular neurons specific',\n",
       "  'model robot approach learn autonomous robots semantic object design service',\n",
       "  'patients treatment cancer survival data clinical model study score analyse',\n",
       "  'model enos expression endothelial identify level regulate mirnas approach data',\n",
       "  'distance time result document quantization algorithm edit measurements cluster problem',\n",
       "  'model brain neural network spike signal activity neurons cortical dynamics',\n",
       "  'microbiome sample study microbial sequence diversity human associate data microbiota',\n",
       "  'model time level dynamic network provide graph energy approach errors',\n",
       "  'flow image cardiac pulmonary patients mean volume artery compute internal',\n",
       "  'position uncertainty gaussian function float temperature measure estimate excursion perimeter',\n",
       "  'mesh brain error motor imagery process element feedback activity finite',\n",
       "  'neural oscillations power activity network aperiodic data measure approach oscillatory',\n",
       "  'data patients research base model health result methods study privacy',\n",
       "  'data pattern applications workflows cells include workflow development large research',\n",
       "  'data workflow learn scientific network compute model science machine resources',\n",
       "  'data study pregnancy model women patients measure clinical result dose',\n",
       "  'genome ecdna sequence cancer identify genes read data result tumor',\n",
       "  'model propose problem data test sample algorithm consider function base',\n",
       "  'atlas brain atlases brainstem texture alignment memoryless essential reference define',\n",
       "  'beta attention neural work alpha test response relate memory time',\n",
       "  'data model time base propose learn approach build real present',\n",
       "  'study time disease model change network variability data cycle circadian',\n",
       "  'model text task sentence propose entity train entities information base',\n",
       "  'trust causal autonomous distribute vehicles scientific robots pose argue discovery',\n",
       "  'cells model financial study covid analysis lead base human abstracttext',\n",
       "  'data learn base network model time process result methods provide'],\n",
       " 40: ['data study cells bacterial analysis cell microbial responses increase sequence',\n",
       "  'model data learn methods train base result participants machine performance',\n",
       "  'cod graph capacity information study scheme storage model rate index',\n",
       "  'network robustness neural adversarial layer certify attack function general model',\n",
       "  'cell type brain regulatory single neurons gene human methylation specific',\n",
       "  'model learn performance improve sample approach number physics techniques object',\n",
       "  'patients dose treatment cancer study clinical data imrt therapy function',\n",
       "  'expression enos endothelial identify regulate mirnas events regulation approach leene',\n",
       "  'distance time edit cluster algorithm problem algorithms approximation string improve',\n",
       "  'model cortex information cortical result activity study data time signal',\n",
       "  'microbiome sample microbial study sequence diversity human associate data microbiota',\n",
       "  'model network neural spike scale brain dynamics neurons population method',\n",
       "  'flow image pulmonary cardiac patients mean dynamic internal artery bias',\n",
       "  'measure perimeter condition wildfire wind cause database problems graph santa',\n",
       "  'brain stop control signal disorder autism motor imagery error activity',\n",
       "  'neural network oscillations power activity cod data scheme oscillatory result',\n",
       "  'data patients methods research result patient base clinical time model',\n",
       "  'pattern text applications data base network theme workflows model information',\n",
       "  'data workflow learn scientific network compute science scale time model',\n",
       "  'model time data event consumption score prednisone analysis random graph',\n",
       "  'ecdna cancer genome identify tumor read oncogene amplification genomic genes',\n",
       "  'propose problem sample algorithm data model base result number function',\n",
       "  'scale rotation curve image diffusion smooth mssr heterogeneity model fibers',\n",
       "  'data position map gaussian work function uncertainty field time scale',\n",
       "  'model approach data learn base time demonstrate propose robot point',\n",
       "  'variability study disease temperature circadian cycle associate time rhythms body',\n",
       "  'model sentence text entity network framework systems entities data level',\n",
       "  'mesh protein cellular process surface distribute simulations diffusion electron enable',\n",
       "  'cells model study analysis financial data covid base liver lead',\n",
       "  'model network base data learn process result provide time methods',\n",
       "  'sequence model bootstrap test variants prediction genome single variation coverage',\n",
       "  'document minimal begin amssymb wasysym mathrsfs documentclass upgreek setlength oddsidemargin',\n",
       "  'build data time applications energy model learn design network process',\n",
       "  'specification robust myogenic lead hipsc myogenesis hipscs cellular temporal differentiation',\n",
       "  'test model consider distribution study mixture scan base propose method',\n",
       "  'model estimators effect face human feature method test social study',\n",
       "  'data model biomedical search information health risk query datamed datasets',\n",
       "  'data study pregnancy women model patients disease flow clinical measure',\n",
       "  'prediction task attention alpha novas stationary shift attentional spatial target',\n",
       "  'causal data different work trust right documentation structure practice social'],\n",
       " 50: ['taxonomy instance motifs topic extreme direction automatic construction paradigm selection',\n",
       "  'model data methods learn result base bias participants datasets performance',\n",
       "  'cod scheme rate capacity graph problem distribute information channel network',\n",
       "  'network robustness neural adversarial certify approximation attack model general propose',\n",
       "  'interactive protocol varepsilon reward channel base nucleus snapatac single edit',\n",
       "  'neural model improve systems control ell_ controller learn build network',\n",
       "  'patients cancer treatment survival clinical imrt dose data study trial',\n",
       "  'endothelial expression identify network enos regulate regulation approach lncrnas function',\n",
       "  'distance time edit result quantization cluster measurements scheme string algorithm',\n",
       "  'right problem information model provide trust result query algorithm time',\n",
       "  'microbiome study human sample sequence data microbial microbiota associate include',\n",
       "  'model neural network spike neurons brain scale plasticity dynamics computational',\n",
       "  'flow patients image cardiac volume pulmonary fraction mean ratio correlation',\n",
       "  'perimeter condition wind measure wildfire santa gradient free result predict',\n",
       "  'brain curve rotation scale signal diffusion mssr heterogeneity fibers control',\n",
       "  'pulmonary data shunt convex level catheterization quantification compute umap control',\n",
       "  'image data flow automate algorithm performance study cardiac change clinical',\n",
       "  'pattern text information data network base mine propose study type',\n",
       "  'workflow learn network data infrastructure chase earth distribute cyclopeptides approach',\n",
       "  'measure information effect symptoms study censorship obstruct approach text variation',\n",
       "  'model think skip train seed semantic sentence relapse provide data',\n",
       "  'propose data model algorithm sample problem base result methods function',\n",
       "  'label train microbiome machine kegg best discipline supervise entry learn',\n",
       "  'position gaussian field function size uncertainty map data scale time',\n",
       "  'data model approach learn time demonstrate base method propose dynamics',\n",
       "  'data study model women patients disease clinical time dose pregnancy',\n",
       "  'entity model text entities sentence bioner level different systems base',\n",
       "  'protein mesh cellular surface ligand diffusion process electron simulations realistic',\n",
       "  'liver model cells financial time lead analysis population study base',\n",
       "  'network model learn base data time process result methods algorithm',\n",
       "  'model learn robots data autonomous result robot algorithms drive train',\n",
       "  'document minimal graph mathrsfs amsmath amsfonts amsbsy upgreek setlength oddsidemargin',\n",
       "  'model learn build data network design applications base energy propose',\n",
       "  'post government chinese specification hipsc social claim media regime party',\n",
       "  'test model consider distribution problem mixture study base sample asymptotic',\n",
       "  'graph random model communities geometric epsilon connectivity approximation provide glucose',\n",
       "  'model score risk clinical screen scale patients classification adhd sentiment',\n",
       "  'model data pregnancy study women survival risk outcomes result compare',\n",
       "  'neural oscillations activity power oscillatory record frequency cortical feature relate',\n",
       "  'causal model activity neural methods dynamics different provide autonomous data',\n",
       "  'data patients research health model privacy methods risk base study',\n",
       "  'time energy level approach propose applications approximate design number process',\n",
       "  'genome sequence ecdna cancer identify data genes read sample result',\n",
       "  'data time workflow systems model compute provide learn scientific applications',\n",
       "  'pain cod facial automatically learn performance train human classifiers vision',\n",
       "  'rods regions differences highly reveal photoreceptors con wide cone development',\n",
       "  'cell type brain neurons regulatory neuronal single methylation cellular gene',\n",
       "  'microbiome microbial sample study diversity associate sequence data result disease',\n",
       "  'model human stop effect spindle feature spindles process synaptic face',\n",
       "  'model bootstrap prediction test time series intervals free propose base']}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'Topic 0: microbiome microbial sample study sequence data diversity associate human microbiota',\n",
       "  'value': 0},\n",
       " {'label': 'Topic 1: data model patients health flow methods result base risk research',\n",
       "  'value': 1},\n",
       " {'label': 'Topic 2: data model learn network base information propose cod provide approach',\n",
       "  'value': 2},\n",
       " {'label': 'Topic 3: network robustness neural model base adversarial certify methods time state',\n",
       "  'value': 3},\n",
       " {'label': 'Topic 4: cell type brain cells human gene expression single neurons regulatory',\n",
       "  'value': 4},\n",
       " {'label': 'Topic 5: learn model time network propose approach process base result state',\n",
       "  'value': 5},\n",
       " {'label': 'Topic 6: patients data study model clinical cancer treatment measure result survival',\n",
       "  'value': 6},\n",
       " {'label': 'Topic 7: genome data sequence ecdna identify cancer patients high read genes',\n",
       "  'value': 7},\n",
       " {'label': 'Topic 8: distance graph algorithm result scheme problem bound approximation time measurements',\n",
       "  'value': 8},\n",
       " {'label': 'Topic 9: model data neural time method brain network base test result',\n",
       "  'value': 9}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#our current labels\n",
    "[{'label' : f'Topic {topic}: {top_words[10][topic]}', 'value' : topic} for topic in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['microbiome',\n",
       " 'data',\n",
       " 'data',\n",
       " 'network',\n",
       " 'cell',\n",
       " 'learn',\n",
       " 'patients',\n",
       " 'genome',\n",
       " 'distance',\n",
       " 'model']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_topics_list(models['10'], names, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{10: ['microbiome',\n",
       "  'data',\n",
       "  'data',\n",
       "  'network',\n",
       "  'cell',\n",
       "  'learn',\n",
       "  'patients',\n",
       "  'genome',\n",
       "  'distance',\n",
       "  'model'],\n",
       " 20: ['microbiome',\n",
       "  'model',\n",
       "  'model',\n",
       "  'network',\n",
       "  'cell',\n",
       "  'model',\n",
       "  'patients',\n",
       "  'level',\n",
       "  'distance',\n",
       "  'model',\n",
       "  'sample',\n",
       "  'model',\n",
       "  'flow',\n",
       "  'mesh',\n",
       "  'data',\n",
       "  'network',\n",
       "  'data',\n",
       "  'data',\n",
       "  'data',\n",
       "  'time'],\n",
       " 30: ['microbial',\n",
       "  'model',\n",
       "  'cod',\n",
       "  'network',\n",
       "  'cell',\n",
       "  'model',\n",
       "  'patients',\n",
       "  'model',\n",
       "  'distance',\n",
       "  'model',\n",
       "  'microbiome',\n",
       "  'model',\n",
       "  'flow',\n",
       "  'position',\n",
       "  'mesh',\n",
       "  'neural',\n",
       "  'data',\n",
       "  'data',\n",
       "  'data',\n",
       "  'data',\n",
       "  'genome',\n",
       "  'model',\n",
       "  'atlas',\n",
       "  'beta',\n",
       "  'data',\n",
       "  'study',\n",
       "  'model',\n",
       "  'trust',\n",
       "  'cells',\n",
       "  'data'],\n",
       " 40: ['data',\n",
       "  'model',\n",
       "  'cod',\n",
       "  'network',\n",
       "  'cell',\n",
       "  'model',\n",
       "  'patients',\n",
       "  'expression',\n",
       "  'distance',\n",
       "  'model',\n",
       "  'microbiome',\n",
       "  'model',\n",
       "  'flow',\n",
       "  'measure',\n",
       "  'brain',\n",
       "  'neural',\n",
       "  'data',\n",
       "  'pattern',\n",
       "  'data',\n",
       "  'model',\n",
       "  'ecdna',\n",
       "  'propose',\n",
       "  'scale',\n",
       "  'data',\n",
       "  'model',\n",
       "  'variability',\n",
       "  'model',\n",
       "  'mesh',\n",
       "  'cells',\n",
       "  'model',\n",
       "  'sequence',\n",
       "  'document',\n",
       "  'build',\n",
       "  'specification',\n",
       "  'test',\n",
       "  'model',\n",
       "  'data',\n",
       "  'data',\n",
       "  'prediction',\n",
       "  'causal'],\n",
       " 50: ['taxonomy',\n",
       "  'model',\n",
       "  'cod',\n",
       "  'network',\n",
       "  'interactive',\n",
       "  'neural',\n",
       "  'patients',\n",
       "  'endothelial',\n",
       "  'distance',\n",
       "  'right',\n",
       "  'microbiome',\n",
       "  'model',\n",
       "  'flow',\n",
       "  'perimeter',\n",
       "  'brain',\n",
       "  'pulmonary',\n",
       "  'image',\n",
       "  'pattern',\n",
       "  'workflow',\n",
       "  'measure',\n",
       "  'model',\n",
       "  'propose',\n",
       "  'label',\n",
       "  'position',\n",
       "  'data',\n",
       "  'data',\n",
       "  'entity',\n",
       "  'protein',\n",
       "  'liver',\n",
       "  'network',\n",
       "  'model',\n",
       "  'document',\n",
       "  'model',\n",
       "  'post',\n",
       "  'test',\n",
       "  'graph',\n",
       "  'model',\n",
       "  'model',\n",
       "  'neural',\n",
       "  'causal',\n",
       "  'data',\n",
       "  'time',\n",
       "  'genome',\n",
       "  'data',\n",
       "  'pain',\n",
       "  'rods',\n",
       "  'cell',\n",
       "  'microbiome',\n",
       "  'model',\n",
       "  'model']}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is our temporary dictionary that we can update in the future\n",
    "temp_dict = {\n",
    "    10 : display_topics_list(models['10'], names, 1),\n",
    "    20 : display_topics_list(models['20'], names, 1),\n",
    "    30 : display_topics_list(models['30'], names, 1),\n",
    "    40 : display_topics_list(models['40'], names, 1),\n",
    "    50 : display_topics_list(models['50'], names, 1)\n",
    "}\n",
    "temp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'microbiome', 'value': 0},\n",
       " {'label': 'data', 'value': 1},\n",
       " {'label': 'data', 'value': 2},\n",
       " {'label': 'network', 'value': 3},\n",
       " {'label': 'cell', 'value': 4},\n",
       " {'label': 'learn', 'value': 5},\n",
       " {'label': 'patients', 'value': 6},\n",
       " {'label': 'genome', 'value': 7},\n",
       " {'label': 'distance', 'value': 8},\n",
       " {'label': 'model', 'value': 9}]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[{'label' : f'{temp_dict[10][topic]}', 'value' : topic} for topic in range(10)]\n",
    "#[{'label' : f'{temp_dict[10][topic]}', 'value' : topic} for topic in range(value)]\n",
    "#[{'label' : f'Topic {topic}: {top_words[value][topic]}', 'value' : topic} for topic in range(value)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:8050/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [11/Jan/2022 11:42:20] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [11/Jan/2022 11:42:21] \"\u001b[37mGET /_dash-dependencies HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [11/Jan/2022 11:42:21] \"\u001b[37mGET /_dash-layout HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [11/Jan/2022 11:42:21] \"\u001b[37mGET /_favicon.ico?v=2.0.0 HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [11/Jan/2022 11:42:21] \"\u001b[37mGET /_dash-component-suites/dash/dcc/async-dropdown.js HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [11/Jan/2022 11:42:21] \"\u001b[37mGET /_dash-component-suites/dash/dcc/async-graph.js HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [11/Jan/2022 11:42:21] \"\u001b[36mGET /_dash-component-suites/dash/dcc/async-plotlyjs.js HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [11/Jan/2022 11:42:21] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [11/Jan/2022 11:42:22] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [11/Jan/2022 11:42:22] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [11/Jan/2022 11:42:31] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [11/Jan/2022 11:42:31] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [11/Jan/2022 11:42:31] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [11/Jan/2022 11:42:37] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [11/Jan/2022 11:42:37] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [11/Jan/2022 12:16:20] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [11/Jan/2022 12:16:21] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "threshold = .1\n",
    "app = dash.Dash(__name__, external_stylesheets=[dbc.themes.DARKLY])\n",
    "\n",
    "app.layout = html.Div([\n",
    "  dbc.Row([\n",
    "      dcc.Dropdown(\n",
    "        id='graph-dropdown',\n",
    "        placeholder='select number of LDA topics',\n",
    "        options=[{'label' : f'{i} Topic Model', 'value' : i} for i in range(10, 60, 10)],\n",
    "        style={\n",
    "          'color' : 'black',\n",
    "          'background-color' : '#666699',\n",
    "          'width' : '200%',\n",
    "          'align-items' : 'left',\n",
    "          'justify-content' : 'left',\n",
    "          'padding-left' : '15px'\n",
    "        },\n",
    "        value=10\n",
    "      )\n",
    "  ]),\n",
    "  dbc.Row([\n",
    "    dbc.Col(html.Div([\n",
    "      dcc.Graph(\n",
    "        id = 'graph',\n",
    "        figure = figs[.1][10]\n",
    "      )\n",
    "      ],\n",
    "      style={\n",
    "        'height' : '100vh',\n",
    "        'overflow-y' : 'scroll'\n",
    "      }\n",
    "    )\n",
    "    ),\n",
    "      dbc.Col(html.Div([dbc.Col([\n",
    "        dcc.Dropdown(\n",
    "          id='dropdown_menu',\n",
    "          placeholder='Select a topic',\n",
    "          #THIS IS THE LINE OF CODE WE ARE UPDATING FOR OUR TEMPORARY TOPICS\n",
    "          #options=[{'label' : f'Topic {topic}: {top_words[10][topic]}', 'value' : topic} for topic in range(10)],\n",
    "          options=[{'label' : f'{temp_dict[10][topic]}', 'value' : topic} for topic in range(10)],\n",
    "          style={\n",
    "            'color' : 'black',\n",
    "            'background-color' : 'white'\n",
    "          }\n",
    "        ),\n",
    "        dcc.Dropdown(\n",
    "          id='researcher-dropdown',\n",
    "          placeholder='Select Researchers',\n",
    "          options=[{'label' : f'{researcher}', 'value' : f'{researcher}'} for researcher in set(author_list)],\n",
    "          style={\n",
    "            'color' : 'black',\n",
    "            'background-color' : 'white'\n",
    "          }\n",
    "        )]),\n",
    "        dbc.Col(\n",
    "          dcc.Dropdown(\n",
    "            id='word-search',\n",
    "            placeholder='Search by word',\n",
    "            options=[{'label' : word, 'value' : word} for word in names],\n",
    "            style={\n",
    "              'color' : 'black',\n",
    "              'background-color' : 'white'\n",
    "            },\n",
    "            value=[],\n",
    "            multi=True\n",
    "          )\n",
    "        ),\n",
    "        html.Div(\n",
    "          id='paper_container', \n",
    "          children=[\n",
    "            html.P(\n",
    "              children=['Top 5 Papers'],\n",
    "              id='titles_and_authors', \n",
    "              draggable=False, \n",
    "              style={\n",
    "                #'font-size' :'150%',\n",
    "                'font-family' : 'Verdana'\n",
    "              }\n",
    "            ),\n",
    "          ],\n",
    "        ),\n",
    "      ], \n",
    "        style={\n",
    "          'height' : '100vh',\n",
    "          'overflow-y' : 'scroll'\n",
    "        }\n",
    "      )\n",
    "      )\n",
    "    ]\n",
    "  )]\n",
    ")\n",
    "\n",
    "@app.callback(\n",
    "  Output('titles_and_authors', 'children'),\n",
    "  Output('researcher-dropdown', 'options'),\n",
    "  Input('dropdown_menu', 'value'),\n",
    "  Input('graph-dropdown', 'value'),\n",
    "  Input('researcher-dropdown', 'value'),\n",
    "  Input('word-search', 'value')\n",
    ")\n",
    "def update_p(topic, num_topics, author, words):\n",
    "  if len(words) != 0:\n",
    "    doc_vec = np.zeros((1, len(names)))\n",
    "    for word in words:\n",
    "      doc_vec[0][locations[word]] += 1\n",
    "    relations = np.round(models[f'{num_topics}'].transform(doc_vec), 5).tolist()[0]\n",
    "    pairs = [(i, relation) for i, relation in enumerate(relations)]\n",
    "    pairs.sort(reverse=True, key=lambda x: x[1])\n",
    "    to_return = [[html.Br(), f'Topic{pair[0]}: {pair[1]}', html.Br()] for pair in pairs]\n",
    "    return list(chain(*to_return)), [{'label' : f'{researcher}', 'value' : f'{researcher}'} for researcher in set(author_list)]\n",
    "\n",
    "  if topic == None and author == None:\n",
    "    return ['Make a selection'], [{'label' : f'{researcher}', 'value' : f'{researcher}'} for researcher in set(author_list)]\n",
    "\n",
    "  if topic != None and author == None:\n",
    "    df = tops[threshold][num_topics][topic]\n",
    "    df_authors = df.author.unique()\n",
    "    max_vals = df.groupby('author').max()[f'{topic}_relevance']\n",
    "\n",
    "    to_return = [[f'{name}:', html.Br(), \n",
    "      f'{df[df[f\"{topic}_relevance\"] == max_vals.loc[name]][\"title\"].to_list()[0]}',\n",
    "      html.Details([html.Summary('Abstract'),\n",
    "                    html.Div(combined[combined.title == f'{df[df[f\"{topic}_relevance\"] == max_vals.loc[name]][\"title\"].to_list()[0]}'].abstract)],\n",
    "                    style={\n",
    "                      'font-size' :'80%',\n",
    "                      'font-family' : 'Verdana'}),\n",
    "      html.Br()] for i, name in enumerate(max_vals.index)]\n",
    "    return list(chain(*to_return)), [{'label' : f'{researcher}', 'value' : f'{researcher}'} for researcher in tops[threshold][num_topics][topic].author.unique()]\n",
    "\n",
    "  if topic == None and author != None:\n",
    "    to_return = []\n",
    "    for topic_num in range(num_topics):\n",
    "      df = tops[threshold][num_topics][topic_num]\n",
    "      if author in df.author.unique():\n",
    "        max_vals = df.groupby('author').max()[f'{topic_num}_relevance']\n",
    "  \n",
    "        to_return.append([f'Topic {topic_num}:', html.Br(), \n",
    "          f'{df[df[f\"{topic_num}_relevance\"] == max_vals.loc[author]][\"title\"].to_list()[0]}', \n",
    "          html.Details([html.Summary('Abstract'), \n",
    "                        html.Div(combined[combined.title == f'{df[df[f\"{topic_num}_relevance\"] == max_vals.loc[author]][\"title\"].to_list()[0]}'].abstract)],\n",
    "                        style={\n",
    "                          'font-size' :'80%',\n",
    "                          'font-family' : 'Verdana'},\n",
    "                        ),\n",
    "          html.Br()])\n",
    "    return list(chain(*to_return)), [{'label' : f'{researcher}', 'value' : f'{researcher}'} for researcher in set(author_list)]\n",
    "\n",
    "  if topic != None and author != None:\n",
    "    df = tops[threshold][num_topics][topic]\n",
    "    df = df[df['author'] == author]\n",
    "    df.sort_values(by=f'{topic}_relevance', ascending=False, inplace=True)\n",
    "    titles = df.head(10)['title'].to_list()\n",
    "    \n",
    "    to_return = [\n",
    "      [f'{i} : {title}', \n",
    "      html.Details([html.Summary('Abstract'), \n",
    "                    html.Div(combined[combined.title == title].abstract)], \n",
    "                    style={\n",
    "                      'font-size' :'80%',\n",
    "                      'font-family' : 'Verdana'}), \n",
    "      html.Br()] for i, title in enumerate(titles)]\n",
    "    return list(chain(*to_return)), [{'label' : f'{researcher}', 'value' : f'{researcher}'} for researcher in tops[threshold][num_topics][topic].author.unique()]\n",
    "    \n",
    "\n",
    "\n",
    "@app.callback(\n",
    "  [Output('graph', 'figure'), Output('dropdown_menu', 'options')],\n",
    "  [Input('graph-dropdown', 'value'), Input('dropdown_menu', 'value'), Input('researcher-dropdown', 'value'), Input('word-search', 'value')],\n",
    "  State('graph', 'figure')\n",
    ")\n",
    "def update_graph(value, topic, author, words, previous_fig):\n",
    "  if len(previous_fig['data'][0]['node']['color']) != value + 108:\n",
    "    figs[threshold][value].update_traces(node = dict(color = ['#666699' for i in range(len(labels[value]))]), link = dict(color = ['rgba(204, 204, 204, .5)' for i in range(len(sources[threshold][value]))]))\n",
    "    #return figs[threshold][value], [{'label' : f'Topic {topic}: {top_words[value][topic]}', 'value' : topic} for topic in range(value)]\n",
    "    return figs[threshold][value], [{'label' : f'{temp_dict[value][topic]}', 'value' : topic} for topic in range(value)]\n",
    "\n",
    "  if len(words) != 0:\n",
    "    doc_vec = np.zeros((1, len(names)))\n",
    "    for word in words:\n",
    "      doc_vec[0][locations[word]] += 1\n",
    "    relations = np.round(models[f'{value}'].transform(doc_vec), 3).tolist()[0]\n",
    "    opacity = {(i+108) : relation for i, relation in enumerate(relations) if relation > .1}\n",
    "    node_colors = ['#666699' if (i not in opacity.keys()) else f'rgba(255, 255, 0, {opacity[i]})' for i in range(len(labels[value]))]\n",
    "    valid_targets = [positions[value][f'Topic{i-108}'] for i in opacity.keys()]\n",
    "    link_colors = ['rgba(204, 204, 204, .5)' if target not in valid_targets else f'rgba(255, 255, 0, .5)' for target in targets[threshold][value]]\n",
    "    figs[threshold][value].update_traces(node = dict(color = node_colors), link = dict(color = link_colors)),\n",
    "    #return figs[threshold][value], [{'label' : f'Topic {topic}: {top_words[value][topic]}', 'value' : topic} for topic in range(value)]\n",
    "    return figs[threshold][value], [{'label' : f'{temp_dict[value][topic]}', 'value' : topic} for topic in range(value)]\n",
    "\n",
    "\n",
    "  if topic == None and author == None:\n",
    "    figs[threshold][value].update_traces(node = dict(color = ['#666699' for i in range(len(labels[value]))]), link = dict(color = ['rgba(204, 204, 204, .5)' for i in range(len(sources[threshold][value]))]))\n",
    "    #return figs[threshold][value], [{'label' : f'Topic {topic}: {top_words[value][topic]}', 'value' : topic} for topic in range(value)]\n",
    "    return figs[threshold][value], [{'label' : f'{temp_dict[value][topic]}', 'value' : topic} for topic in range(value)]\n",
    "  \n",
    "  if topic != None and author == None:\n",
    "    node_colors = ['#666699' if (i != positions[value][f'Topic{topic}']) else '#ffff00' for i in range(len(labels[value]))]\n",
    "    link_colors = ['rgba(204, 204, 204, .5)' if target != positions[value][f'Topic{topic}'] else 'rgba(255, 255, 0, .5)' for target in targets[threshold][value]]\n",
    "    figs[threshold][value].update_traces(node = dict(color = node_colors), link = dict(color = link_colors))\n",
    "    #return figs[threshold][value], [{'label' : f'Topic {topic}: {top_words[value][topic]}', 'value' : topic} for topic in range(value)]\n",
    "    return figs[threshold][value], [{'label' : f'{temp_dict[value][topic]}', 'value' : topic} for topic in range(value)]\n",
    "\n",
    "  if topic == None and author != None:\n",
    "    node_colors = ['#666699' if (i != positions[value][author]) else '#ffff00' for i in range(len(labels[value]))]\n",
    "    link_colors = ['rgba(204, 204, 204, .5)' if source != positions[value][author] else 'rgba(255, 255, 0, .5)' for source in sources[threshold][value]]\n",
    "    figs[threshold][value].update_traces(node = dict(color = node_colors), link = dict(color = link_colors))\n",
    "    #return figs[threshold][value], [{'label' : f'Topic {topic}: {top_words[value][topic]}', 'value' : topic} for topic in range(value)]\n",
    "    return figs[threshold][value], [{'label' : f'{temp_dict[value][topic]}', 'value' : topic} for topic in range(value)]\n",
    "\n",
    "  if topic != None and author != None:\n",
    "    node_colors = ['#666699' if (i != positions[value][author] and i != positions[value][f'Topic{topic}']) else '#ffff00' for i in range(len(labels[value]))]\n",
    "    link_colors = ['rgba(204, 204, 204, .5)' if (source != positions[value][author] or target != positions[value][f'Topic{topic}']) else 'rgba(255, 255, 0, .5)' for source, target in zip(sources[threshold][value], targets[threshold][value])]\n",
    "    figs[threshold][value].update_traces(node = dict(color = node_colors), link = dict(color = link_colors))\n",
    "    #return figs[threshold][value], [{'label' : f'Topic {topic}: {top_words[value][topic]}', 'value' : topic} for topic in range(value)]\n",
    "    return figs[threshold][value], [{'label' : f'{temp_dict[value][topic]}', 'value' : topic} for topic in range(value)]\n",
    "\n",
    "@app.callback(\n",
    "  Output('researcher-dropdown', 'value'),\n",
    "  Input('dropdown_menu', 'value'),\n",
    "  State('dropdown_menu', 'value')\n",
    ")\n",
    "def reset_author(topic, previous):\n",
    "  if topic != previous:\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "app.run_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
